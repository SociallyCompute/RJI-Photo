Traceback (most recent call last):
  File "bkgd_quality_trainer.py", line 449, in <module>
    run(dataset)
  File "bkgd_quality_trainer.py", line 405, in run
    train_data_function(train, epochs, prev_model, dataset, label_dict, model_name)
  File "bkgd_quality_trainer.py", line 332, in train_data_function
    for i, data in enumerate(train_loader,0):
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 346, in __next__
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "bkgd_quality_trainer.py", line 54, in __getitem__
    original_tuple = super(ImageFolderWithPathsAndRatings, self).__getitem__(index)
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torchvision/datasets/folder.py", line 140, in __getitem__
    sample = self.transform(sample)
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 61, in __call__
    img = t(img)
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torchvision/transforms/transforms.py", line 92, in __call__
    return F.to_tensor(pic)
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torchvision/transforms/functional.py", line 99, in to_tensor
    return img.float().div(255)
KeyboardInterrupt
Traceback (most recent call last):
  File "classifier.py", line 249, in <module>
    run(epochs)
  File "classifier.py", line 227, in run
    train_vgg(train_loader, epochs, pic_label_dict, tag_mapping)
  File "classifier.py", line 213, in train_vgg
    torch.save(vgg16.state_dict(), 'models/CLASSIFICATION_Feb11_All_AVA_only_training.pt')
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/serialization.py", line 260, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/serialization.py", line 183, in _with_file_like
    f = open(f, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'models/CLASSIFICATION_Feb11_All_AVA_only_training.pt'
Traceback (most recent call last):
  File "classifier.py", line 250, in <module>
    run(epochs)
  File "classifier.py", line 228, in run
    train_vgg(train_loader, epochs, pic_label_dict, tag_mapping)
  File "classifier.py", line 213, in train_vgg
    torch.save(vgg16.state_dict(), '../neural_nets/models/CLASSIFICATION_Feb17_All_AVA_only_training.pt')
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/serialization.py", line 260, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/serialization.py", line 183, in _with_file_like
    f = open(f, mode)
FileNotFoundError: [Errno 2] No such file or directory: '../neural_nets/models/CLASSIFICATION_Feb17_All_AVA_only_training.pt'
Traceback (most recent call last):
  File "bkgd_quality_trainer_vgg16.py", line 462, in <module>
    run(dataset)
  File "bkgd_quality_trainer_vgg16.py", line 405, in run
    label_dict = get_ava_labels()
  File "bkgd_quality_trainer_vgg16.py", line 230, in get_ava_labels
    pic_label_dict[picture_name] = np.asarray(aesthetic_values).argmax()
  File "/home/newmatt/rji3/lib/python3.7/site-packages/numpy/core/_asarray.py", line 85, in asarray
    return array(a, dtype, copy=False, order=order)
KeyboardInterrupt
Traceback (most recent call last):
  File "bkgd_quality_trainer_vgg16.py", line 353, in train_data_function
    for i, (data, label) in enumerate(train_loader,0):
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 346, in __next__
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "bkgd_quality_trainer_vgg16.py", line 108, in __getitem__
    sample = self.pil_loader(path) #transform Image into Tensor
  File "bkgd_quality_trainer_vgg16.py", line 63, in pil_loader
    image = image.convert('RGB')
  File "/home/newmatt/rji3/lib/python3.7/site-packages/PIL/Image.py", line 930, in convert
    self.load()
  File "/home/newmatt/rji3/lib/python3.7/site-packages/PIL/ImageFile.py", line 249, in load
    "(%d bytes not processed)" % len(b)
OSError: image file is truncated (18 bytes not processed)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "bkgd_quality_trainer_vgg16.py", line 462, in <module>
    run(dataset)
  File "bkgd_quality_trainer_vgg16.py", line 417, in run
    train_data_function(train, epochs, prev_model, dataset, label_dict, model_name)
  File "bkgd_quality_trainer_vgg16.py", line 375, in train_data_function
    (data, label) = train_loader
ValueError: too many values to unpack (expected 2)
Traceback (most recent call last):
  File "bkgd_quality_trainer_vgg16.py", line 353, in train_data_function
    for i, (data, label) in enumerate(train_loader,0):
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 346, in __next__
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "bkgd_quality_trainer_vgg16.py", line 108, in __getitem__
    sample = self.pil_loader(path) #transform Image into Tensor
  File "bkgd_quality_trainer_vgg16.py", line 63, in pil_loader
    image = image.convert('RGB')
  File "/home/newmatt/rji3/lib/python3.7/site-packages/PIL/Image.py", line 930, in convert
    self.load()
  File "/home/newmatt/rji3/lib/python3.7/site-packages/PIL/ImageFile.py", line 249, in load
    "(%d bytes not processed)" % len(b)
OSError: image file is truncated (16 bytes not processed)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "bkgd_quality_trainer_vgg16.py", line 462, in <module>
    run(dataset)
  File "bkgd_quality_trainer_vgg16.py", line 417, in run
    train_data_function(train, epochs, prev_model, dataset, label_dict, model_name)
  File "bkgd_quality_trainer_vgg16.py", line 375, in train_data_function
    (data, label) = train_loader
ValueError: too many values to unpack (expected 2)
Traceback (most recent call last):
  File "bkgd_quality_trainer_vgg16.py", line 9, in <module>
    import torch
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/__init__.py", line 322, in <module>
    import torch.quantization
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/quantization/__init__.py", line 2, in <module>
    from .quantize import *  # noqa: F401
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/quantization/quantize.py", line 12, in <module>
    from .default_mappings import (DEFAULT_DYNAMIC_MODULE_MAPPING,
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/quantization/default_mappings.py", line 5, in <module>
    import torch.nn.intrinsic.quantized as nniq
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 818, in get_code
  File "<frozen importlib._bootstrap_external>", line 917, in get_data
KeyboardInterrupt
Traceback (most recent call last):
  File "bkgd_quality_trainer_vgg16.py", line 353, in train_data_function
    for i, (data, label) in enumerate(train_loader,0):
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 346, in __next__
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "bkgd_quality_trainer_vgg16.py", line 108, in __getitem__
    sample = self.pil_loader(path) #transform Image into Tensor
  File "bkgd_quality_trainer_vgg16.py", line 63, in pil_loader
    image = image.convert('RGB')
  File "/home/newmatt/rji3/lib/python3.7/site-packages/PIL/Image.py", line 930, in convert
    self.load()
  File "/home/newmatt/rji3/lib/python3.7/site-packages/PIL/ImageFile.py", line 249, in load
    "(%d bytes not processed)" % len(b)
OSError: image file is truncated (18 bytes not processed)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "bkgd_quality_trainer_vgg16.py", line 462, in <module>
    run(dataset)
  File "bkgd_quality_trainer_vgg16.py", line 417, in run
    train_data_function(train, epochs, prev_model, dataset, label_dict, model_name)
  File "bkgd_quality_trainer_vgg16.py", line 375, in train_data_function
    (data, label) = train_loader
ValueError: too many values to unpack (expected 2)
Traceback (most recent call last):
  File "bkgd_quality_trainer_resnet50.py", line 353, in train_data_function
    for i, (data, label) in enumerate(train_loader,0):
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 346, in __next__
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/newmatt/rji3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "bkgd_quality_trainer_resnet50.py", line 108, in __getitem__
    sample = self.pil_loader(path) #transform Image into Tensor
  File "bkgd_quality_trainer_resnet50.py", line 63, in pil_loader
    image = image.convert('RGB')
  File "/home/newmatt/rji3/lib/python3.7/site-packages/PIL/Image.py", line 930, in convert
    self.load()
  File "/home/newmatt/rji3/lib/python3.7/site-packages/PIL/ImageFile.py", line 249, in load
    "(%d bytes not processed)" % len(b)
OSError: image file is truncated (12 bytes not processed)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "bkgd_quality_trainer_resnet50.py", line 462, in <module>
    run(dataset)
  File "bkgd_quality_trainer_resnet50.py", line 417, in run
    train_data_function(train, epochs, prev_model, dataset, label_dict, model_name)
  File "bkgd_quality_trainer_resnet50.py", line 375, in train_data_function
    (data, label) = train_loader
ValueError: too many values to unpack (expected 2)
training loss: 0.371381012255038
training accuracy: 36.88925591370916
training loss: 0.9288266814876049
training accuracy: 34.66388527952158
