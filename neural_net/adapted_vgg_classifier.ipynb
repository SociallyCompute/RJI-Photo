{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Quality Assessment\n",
    "-----------------------------------\n",
    "#### Steps:\n",
    "**1:** Read in all the images from the server.\n",
    "\n",
    "**2:** Break the images into smaller categories. This can be accomplished via a K-Means classifier or possibly by running the images through a different CNN beforehand. These categories are then stored and used later. \n",
    "\n",
    "**3:** Determine labels based on the metadata stored in the pictures. For many of the 2017 pictures there is exif, IPTC, and XMP. The differences are as follows:\n",
    "\n",
    "* exif - Stored by camera at the time of the picture. This includes GPS location, pixel count, zoom, etc.\n",
    "* IPTC - Metadata added via an editor. This is generally an older bulk metadata system. While still used it is getting phased out by XMP.\n",
    "* XMP - Metadata that can be applied as an \"xml\"-like file or added in headers and footers of images. It is more clearly defined than IPTC and significantly newer. It also allows for more metadata to be stored. \n",
    "\n",
    "For both IPTC and XMP, they contain information we can use as labels. We have preference tags (code 221) where information has been stored determining which pictures have been preferenced before our work began. In addition to this, the Reynolds Journalism School has informed us that they have color classed about half of the images via PhotoMechanic which is stored in the IPTC and XMP data. These color classes correspond to a quality assessment on a rough range of 0-9. Once labels have been determined we can begin training.\n",
    "\n",
    "**4:** Import VGG-16. This is a very successful pretrained classification model that has shown promise in previous studies attempting similar techniques as us, most notibly NIMA (Neural IMage Assessment) by the perceptron team at Google.AI in 2015.\n",
    "\n",
    "**5:** Once the pretrained model has been imported we can pull the fully connected component off the top and replace it with our own. Initially we will replace it with one total classifier to determine quality but after proof of concept has been established we can supply two different classifiers (1 for technical assessment and 1 for aesthetics). These classifiers shall be constant for each category of picture while just changing the loaded weights for the system. This allows us to adjust our idea of what quality is based on the category it is in. \n",
    "\n",
    "**6:** After the models have provided a technical and aesthetic score we can use the Choquet Integral to aggregate the data together and get a measurement of uncertainty. This shall provide us with a rating and a range both of which can be used to rank photos in a set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard ML/Image Processing imports\n",
    "import numpy as np\n",
    "import math, pandas\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#pytorch imports\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "home = \"../../../../../mnt/md0/mysql-dump-economists/Archives\"#/Fall\"#/Dump\"\n",
    "vgg16 = models.vgg16(pretrained=True) \n",
    "\n",
    "_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have created all our global variables necessary for the classifier and imported all necessary libraries. Next we need to determine the images under test. Once we have gotten access to GPUs we can scale this up. This class extends the ImageFolder and allows us to iterate through and pull all files from inside a home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split the data into training and testing. This is accomplished using this function. The number of pictures used in the training/testing sets is set via the limit_num_pictures variable in this function. Set it to null if you don't want a limit. This will be accomplished once we have access to GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_test(datadir, valid_size = .2):\n",
    "    \n",
    "    # Helper/controller params for checking size\n",
    "    find_size_bounds = False #set to true if you are looking for min/max dims in the current set, false if you want them to be resized\n",
    "    limit_num_pictures = 1000 #set to null if you want no limit\n",
    "    \n",
    "    # load data and apply the transforms on contained pictures\n",
    "    train_data = ImageFolderWithPaths(datadir, transform=_transform)\n",
    "    test_data = ImageFolderWithPaths(datadir, transform=_transform)   \n",
    "    \n",
    "    maxh = 0\n",
    "    minh = 10000\n",
    "    maxw = 0\n",
    "    minw = 10000\n",
    "    if find_size_bounds:\n",
    "        try:\n",
    "            for (i, pic) in enumerate(train_data):\n",
    "                #if we are limiting pics\n",
    "                if limit_num_pictures:\n",
    "                    if i > limit_num_pictures:\n",
    "                        break\n",
    "                print(pic[0].size())\n",
    "                if pic[0].size()[1] > maxw:\n",
    "                    maxw = pic[0].size()[1]\n",
    "                elif pic[0].size()[1] < minw:\n",
    "                    minw = pic[0].size()[1]\n",
    "\n",
    "                if pic[0].size()[2] > maxh:\n",
    "                    maxh = pic[0].size()[2]\n",
    "                elif pic[0].size()[2] < minh:\n",
    "                    minh = pic[0].size()[2]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"error occurred on pic {} number {}\".format(pic, i))\n",
    "    \n",
    "        print(\"Max/min width: {} {}\".format(maxw, minw))\n",
    "        print(\"Max/min height: {} {}\".format(maxh, minh))\n",
    "    \n",
    "    num_pictures = len(train_data)\n",
    "    print(\"Number of pictures in subdirectories: {}\".format(num_pictures))\n",
    "    \n",
    "    # Shuffle pictures and split training set\n",
    "    indices = list(range(num_pictures))\n",
    "    print(\"Head of indices: {}\".format(indices[:10]))\n",
    "    \n",
    "    split = int(np.floor(valid_size * num_pictures))\n",
    "    print(\"Split index: {}\".format(split))\n",
    "    \n",
    "    # may be unnecessary with the choice of sampler below\n",
    "#     np.random.shuffle(indices)\n",
    "#     print(\"Head of shuffled indices: {}\".format(indices[:10]))\n",
    "    \n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    print(\"Size of training set: {}, size of test set: {}\".format(len(train_idx), len(test_idx)))\n",
    "    \n",
    "    # Define samplers that sample elements randomly without replacement\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    # Define data loaders, which allow batching the data, shuffling the data, and \n",
    "    #     loading the data in parallel using multiprocessing workers\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=1)#, num_workers=4)\n",
    "    testloader = torch.utils.data.DataLoader(test_data,\n",
    "                   sampler=test_sampler, batch_size=1)#, num_workers=4)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the dataset loader helper functions we can run the files. We can begin processing. The function run_k_means_files() splits the data for K-Means processing, otherwise we need to run it against the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_means_files():\n",
    "    count = 0\n",
    "    limit = 150\n",
    "    print(\"Percent done: {}%\".format(count/limit*100))\n",
    "    for inputs, labels, paths in trainloader:\n",
    "        print('\\n{}'.format(paths))\n",
    "        print(inputs.size())\n",
    "        mat4d = inputs\n",
    "        mat4d = mat4d[0::2,0::2,:,:]\n",
    "        mat2d = mat4d.resize_((mat4d.shape[1] * mat4d.shape[2]), mat4d.shape[3])\n",
    "        \n",
    "        add_to_list_file(paths[0], mat2d)\n",
    "        count = count + 1\n",
    "        print(\"Percent done: {}%\".format(count/limit*100))\n",
    "        if(count >= limit):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vgg():\n",
    "    training, testing = load_split_train_test(home, .2)\n",
    "    vgg16.eval()\n",
    "    output = vgg16(training)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final cell is running the correct machine. Comment out the one you don't want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_vgg()\n",
    "# run_k_means_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
