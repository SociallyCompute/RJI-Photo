{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard ML/Image Processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, pandas\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#pytorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# no one likes irrelevant warnings\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# root directory where the images are stored\n",
    "data_dir = \"/mnt/md0/reynolds/ava-dataset/\"\n",
    "label_file = \"/mnt/md0/reynolds/ava-dataset/AVA_dataset/AVA.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'953619': 4, '953958': 3, '954184': 5, '954113': 4, '953980': 4, '954175': 5, '953349': 5, '953645': 5, '953897': 5, '953841': 5, '953417': 5, '953777': 5, '953756': 5, '954195': 4, '953903': 5, '954222': 5, '953889': 5, '953844': 5, '954104': 5, '954229': 4, '953550': 5, '953726': 4, '954228': 4, '953750': 4, '954181': 5, '954208': 5, '953810': 5, '954187': 4, '953621': 5, '953348': 4, '953283': 5, '953092': 5, '953751': 5, '954112': 4, '954117': 5, '954116': 5, '953821': 5, '954105': 5, '953019': 4, '953780': 4, '954186': 4, '954119': 5, '954063': 4, '954121': 5, '954066': 4, '954125': 4, '954067': 5, '953582': 5, '954013': 5, '954069': 4, '954071': 4, '954225': 4, '954124': 5, '954218': 5, '953577': 5, '953985': 5, '954141': 5, '954118': 5, '953901': 5, '953794': 4, '953853': 4, '953757': 5, '954130': 4, '953994': 4, '954180': 5, '954014': 4, '954092': 5, '954226': 3, '953946': 5, '953334': 4, '953126': 5, '953245': 4, '954122': 4, '954227': 5, '953097': 6, '953965': 4, '954002': 5, '953327': 4, '954052': 5, '954224': 5, '953863': 5, '953918': 4, '953933': 4, '953630': 4, '953004': 4, '953935': 4, '954219': 5, '953977': 4, '954048': 4, '954183': 4, '953715': 4, '953955': 5, '953785': 6, '954012': 4, '953856': 4, '954127': 5, '954206': 4, '954055': 4, '953981': 3, '954007': 5, '954216': 4, '953824': 5, '954176': 5, '953123': 5, '954133': 4, '953934': 4, '953560': 4, '953979': 5, '954179': 5, '954126': 5, '953802': 5, '954217': 5, '954131': 4, '953356': 4, '954169': 5, '954200': 4, '954220': 4, '954143': 4, '954123': 4, '953113': 5, '954223': 4, '954043': 4, '954022': 4, '953730': 5, '954212': 5, '953945': 5, '954096': 4, '954076': 5, '954178': 5, '954196': 5, '953961': 5, '954214': 5, '954203': 4, '954198': 5, '954011': 5, '953816': 6, '954191': 4, '954188': 5, '953923': 4, '953089': 5, '953401': 5, '771289': 4, '770869': 5, '770556': 4, '770300': 4, '771576': 4, '770637': 5, '771245': 4, '771518': 5, '771439': 4, '771182': 5, '771082': 4, '770779': 4, '771609': 5, '771252': 5, '771392': 4, '771538': 4, '771530': 5, '771628': 4, '771109': 4, '771346': 5, '771355': 4, '771541': 5, '771257': 3, '771637': 4, '770394': 4, '770632': 5, '771582': 4, '771269': 4, '770911': 5, '771351': 5, '771268': 5, '771596': 5, '771123': 4, '770826': 5, '771583': 5, '771189': 4, '770891': 4, '771084': 4, '771436': 4, '771430': 5, '771603': 4, '771522': 4, '771213': 6, '771648': 4, '770992': 5, '771647': 4, '770696': 4, '770993': 5, '771309': 4, '771520': 4, '770642': 3, '771571': 4, '771529': 4, '771185': 4, '770897': 6, '771047': 4, '770684': 4, '771443': 4, '770185': 3, '771459': 5, '771234': 4, '771046': 5, '770638': 4, '771452': 4, '770004': 4, '771483': 4, '771602': 5, '771404': 4, '771254': 4, '770916': 5, '771227': 4, '771466': 5, '771296': 4, '770927': 4, '771300': 5, '770486': 6, '771624': 4, '771533': 4, '771233': 5, '771652': 4, '771030': 4, '771229': 5, '770238': 5, '771203': 4, '769857': 4, '771342': 5, '771186': 3, '771438': 4, '771399': 6, '771027': 4, '769958': 5, '771308': 3, '771645': 4, '771451': 4, '770768': 4, '771131': 4, '771634': 5, '770926': 4, '771556': 5, '771425': 3, '769959': 4, '770809': 4, '771463': 5, '771184': 4, '771547': 5, '771524': 4, '771163': 5, '771408': 4, '770938': 4, '771572': 4, '771400': 4, '771585': 4, '770942': 4, '771650': 4, '770900': 4, '769987': 4, '771111': 6, '771039': 4, '771238': 4, '771447': 4, '770854': 5, '771347': 4, '771376': 4, '771401': 5, '770904': 4, '769730': 4, '771455': 4, '770560': 4, '771651': 4, '771107': 4, '771549': 4, '771149': 5, '771005': 5, '771474': 3, '771593': 5, '770608': 4, '771536': 4, '771172': 5, '771457': 4, '770989': 4, '771249': 4, '769801': 4, '771320': 5, '770568': 4, '770649': 5, '771605': 4, '771108': 4, '771397': 4, '771148': 4, '771601': 3, '770875': 5, '771653': 4, '771557': 4, '771429': 4, '771570': 4, '771294': 4, '770690': 5, '771500': 4, '770671': 4, '769736': 4, '771202': 4, '771632': 4, '771626': 5, '771584': 4, '771411': 4, '771516': 5, '770538': 5, '771122': 4, '771589': 5, '771545': 4, '771523': 4, '771508': 4, '771323': 5, '771502': 4, '771625': 4, '771616': 4, '770437': 4, '771627': 4, '771642': 4, '771497': 5, '930504': 5, '930322': 4, '930473': 4, '930811': 5, '930657': 4, '929988': 4, '930455': 5, '930060': 5, '930705': 5, '930493': 4, '930747': 5, '930073': 4, '930709': 5, '929897': 5, '930695': 4, '930465': 4, '930674': 4, '930324': 4, '930717': 4, '930165': 4, '930790': 3, '930425': 4, '930638': 4, '930194': 5, '930499': 5, '930694': 4, '930219': 5, '930293': 4, '929995': 5, '930734': 4, '930217': 4, '930299': 4, '930336': 4, '930436': 4, '930532': 5, '930095': 4, '930636': 4, '930494': 3, '930808': 4, '930225': 5, '930104': 4, '930254': 5, '930212': 4, '930596': 4, '930286': 5, '930767': 4, '930632': 4, '930759': 4, '929932': 4, '930639': 5, '929914': 4, '930390': 5, '930735': 3, '930476': 5, '930756': 5, '930021': 5, '930379': 5, '930770': 5, '930542': 5, '930236': 5, '930698': 4, '930761': 4, '930703': 4, '930809': 4, '930732': 3, '929777': 4, '929936': 5, '930531': 4, '930065': 5, '930692': 4, '930805': 4, '930433': 4, '930247': 4, '930403': 4, '930626': 4, '930248': 5, '930802': 4, '930716': 3, '930723': 4, '930665': 5, '930389': 4, '930456': 5, '930707': 4, '929918': 4, '930387': 5, '930696': 4, '930401': 5, '929781': 5, '929970': 5, '930594': 5, '930623': 4, '930235': 4, '930590': 4, '930430': 5, '930773': 5, '929981': 3, '930647': 4, '930407': 4, '930799': 3, '930472': 5, '930741': 4, '930783': 4, '930233': 4, '930446': 4, '930644': 5, '930771': 4, '930444': 4, '929810': 5, '930306': 4, '930711': 5, '930660': 5, '930445': 5, '930435': 5, '930600': 4, '930258': 3, '930737': 4, '930727': 4, '930785': 4, '930789': 5, '930758': 4, '930602': 4, '930004': 4, '930742': 4, '930240': 4, '929957': 4, '929843': 4, '930650': 5, '930662': 4, '930630': 4, '930753': 4, '930373': 3, '930769': 5, '930040': 5, '930411': 4, '930587': 5, '930329': 4, '444094': 4, '444771': 3, '445231': 5, '445317': 4, '444309': 4, '443745': 5, '444029': 5, '445235': 4, '443262': 4, '444465': 5, '442503': 4, '445067': 5, '444887': 4, '445144': 4, '444245': 4, '444200': 4, '443773': 4, '444242': 4, '445191': 4, '444777': 4, '444381': 4, '444986': 4, '443926': 4, '445199': 4, '444966': 5, '445201': 4, '444885': 4, '444009': 4, '443092': 4, '444647': 4, '444667': 4, '445194': 4, '444271': 4, '445214': 4, '445270': 4, '444655': 4, '443402': 4, '445339': 4, '444941': 4, '445263': 5, '444281': 5, '445097': 4, '444681': 4, '444395': 4, '443914': 4, '445367': 5, '444910': 5, '444102': 4, '445005': 4, '445337': 4, '445070': 4, '444995': 5, '445046': 4, '444415': 5, '443072': 4, '445312': 3, '443543': 4, '445325': 4, '149173': 5, '149385': 4, '149531': 4, '149172': 4, '149446': 3, '149522': 3, '149492': 4, '148385': 4, '149436': 4, '149537': 3, '148244': 5, '149450': 4, '149087': 4, '148139': 4, '149576': 4, '149547': 4, '149526': 4, '148335': 4, '149500': 4, '149383': 4, '149230': 5, '149257': 4, '149299': 3, '148714': 4, '149112': 4, '148124': 5, '149050': 4, '149427': 4, '149233': 3, '149545': 2, '149428': 5, '148849': 4, '149467': 3, '148587': 5, '149078': 4, '147836': 3, '148702': 4, '149542': 4, '149268': 4, '149512': 4, '149259': 4, '149517': 4, '149459': 4, '149252': 4, '148631': 4, '147792': 4, '149376': 4, '149441': 3, '148768': 4, '149521': 4, '147906': 3, '149574': 4, '149561': 4, '149346': 4, '149249': 4, '149601': 4, '149544': 4, '149199': 4, '148908': 4, '148567': 4, '149236': 4, '149017': 4, '148342': 4, '149552': 4, '149509': 4, '148701': 4, '148347': 4, '149571': 4, '148787': 3, '149570': 4, '149524': 4, '149130': 4, '148523': 4, '149209': 5, '149533': 4, '149553': 7, '149364': 4, '149382': 4, '149386': 5, '149353': 4, '149287': 4, '148490': 4, '148888': 5, '149519': 4, '149462': 3, '149432': 4, '149186': 3, '149234': 4, '149596': 4, '149612': 4, '148584': 4, '147834': 4, '149476': 4, '149586': 4, '148869': 4, '149171': 4, '149248': 4, '149613': 5, '148748': 5, '148533': 5, '149516': 7, '149458': 4, '149258': 4, '149597': 4, '148562': 4, '147786': 3, '149281': 4, '149435': 4, '149185': 4, '149466': 4, '149301': 4, '149564': 4, '149378': 5, '149342': 4, '147924': 4, '149421': 4, '149174': 5, '148296': 3, '148459': 4, '148084': 4, '149433': 4, '149599': 4, '148313': 4, '149508': 4, '148031': 3, '148024': 4, '149343': 3, '149527': 4, '148423': 4, '149551': 4, '148685': 4, '149344': 5, '149296': 4, '148906': 4, '148249': 4, '149365': 4, '149609': 4, '149598': 4, '149086': 3, '149246': 3, '149611': 4, '147815': 4, '148542': 3, '148608': 4, '149610': 4, '149212': 4, '149019': 5, '148916': 4, '149600': 4, '149395': 4, '149503': 4, '149402': 4, '149245': 4, '149313': 5, '149577': 4, '149213': 4, '149373': 4, '149217': 4, '149197': 4, '148656': 4, '149507': 4, '149523': 5, '149253': 5, '147859': 3, '147996': 3, '149556': 5, '149241': 4, '775607': 5, '775599': 4, '775596': 5, '775050': 4, '774585': 4, '774319': 5, '775248': 5, '774463': 4, '775601': 4, '775669': 5, '775708': 4, '774796': 4, '774683': 4, '775651': 4, '775704': 4, '775309': 4, '774139': 5, '775698': 5, '774728': 5, '775041': 5, '775665': 4, '774708': 4, '774687': 5, '775660': 5, '775705': 5, '775662': 4, '775074': 4, '775318': 5, '775431': 4, '775258': 4, '775278': 5, '774062': 4, '775700': 4, '774900': 5, '774524': 5, '774871': 6, '775205': 5, '775280': 4, '775294': 4, '775603': 4, '775713': 4, '775367': 5, '775583': 4, '775072': 4, '775209': 4, '775692': 4, '774643': 4, '774455': 4, '775120': 4, '775302': 5, '774593': 4, '775414': 4, '775712': 5, '775691': 4, '774066': 6, '775268': 4, '775511': 5, '775416': 4, '775415': 4, '775533': 4, '775287': 4, '774510': 5, '775082': 5, '774846': 6, '774453': 4, '775717': 2, '775434': 5, '775683': 4, '775422': 4, '775696': 5, '775539': 4, '775164': 5, '775079': 4, '774884': 4, '775103': 5, '775019': 4, '774591': 4, '775015': 5, '775510': 4, '775517': 4, '774870': 4, '775308': 5, '775259': 4, '775668': 4, '775106': 4, '774396': 3, '775679': 4, '775435': 4, '775007': 4, '775441': 5, '774794': 4, '775545': 5, '775432': 4, '775521': 4, '774992': 3, '775675': 5, '775235': 4, '775711': 4, '775570': 5, '775135': 4, '775250': 5, '774232': 4, '775301': 4, '775424': 3, '775516': 5, '775021': 5, '775694': 4, '775661': 5, '775172': 4, '775690': 4, '775532': 6, '775430': 4, '775418': 4, '775646': 5, '775426': 4, '775544': 3, '775687': 5, '775400': 5, '775591': 4, '775277': 5, '775000': 4, '775702': 6, '774576': 5, '775518': 4, '775658': 4, '775073': 4, '775648': 4, '775552': 4, '775051': 4, '775360': 5, '775659': 4, '775377': 5, '689114': 6, '689637': 6, '689304': 4, '689730': 4, '689735': 5, '689627': 4, '689718': 4, '689766': 4, '688999': 4, '688856': 4, '689470': 4, '689714': 3, '689641': 6, '689645': 4, '689717': 4, '688016': 3, '688008': 5, '689443': 4, '689801': 5, '689626': 4, '688646': 4, '689802': 3, '689748': 4, '688533': 5, '689797': 4, '689787': 4, '689624': 4, '689544': 5, '688752': 4, '689765': 5, '689755': 3, '688205': 5, '689103': 5, '689803': 4, '688612': 4, '689695': 3, '689722': 4, '689596': 5, '689692': 4, '689588': 5, '689715': 5, '689666': 4, '688711': 4, '689784': 5, '689170': 5, '689558': 5, '689694': 5, '689702': 6, '689781': 4, '689651': 3, '689805': 5, '689740': 5, '689780': 5, '688389': 4, '689713': 3, '689262': 4, '689823': 5, '689215': 4, '689440': 5, '689706': 4, '689729': 4, '688820': 4, '689647': 4, '689710': 4, '689738': 6, '689772': 5, '689727': 4, '689542': 4, '689608': 5, '688360': 5, '689796': 4, '688434': 5, '689341': 4, '689762': 6, '689737': 4, '689500': 4, '689534': 5, '689826': 4, '688797': 4, '689244': 5, '688295': 4, '689768': 4, '689639': 4, '689274': 5, '689798': 5, '689699': 4, '689236': 5, '689095': 4, '689818': 6, '689719': 4, '689446': 4, '688548': 4, '689819': 4, '689457': 4, '688874': 4, '689455': 4, '689636': 6, '689071': 4, '689303': 4, '688924': 4, '689625': 5, '689688': 4, '689734': 5, '688679': 4, '689791': 4, '689643': 3, '689812': 4, '688969': 4, '689386': 4, '689620': 5, '689630': 4, '689790': 5, '689338': 4, '688039': 4, '688947': 3, '689509': 4, '689709': 5, '689216': 4, '689698': 3, '689302': 4, '688357': 4, '308886': 3, '307198': 5, '306592': 4, '307079': 6, '308509': 3, '308027': 5, '308404': 5, '306189': 5, '307522': 5, '307582': 6, '308877': 5, '307871': 5, '308880': 4, '308077': 3, '307930': 4, '306313': 5, '308356': 5, '308089': 5, '308466': 4, '308519': 5, '307619': 4, '308836': 5, '307196': 6, '308862': 3, '308117': 4, '306765': 4, '307148': 4, '308672': 4, '307116': 4, '308530': 3, '307939': 4, '308245': 6, '308315': 5, '308647': 3, '308869': 3, '308714': 4, '308680': 5, '308442': 4, '308889': 4, '308555': 4, '308357': 4, '308749': 6, '308795': 4, '308324': 4, '308851': 4, '307267': 6, '308291': 5, '307150': 4, '307606': 4, '308206': 4, '308119': 5, '307793': 4, '308532': 4, '308852': 4, '307726': 6, '308222': 4, '307719': 4, '308678': 5, '308752': 4, '308633': 5, '307007': 4, '307977': 4, '308420': 4, '308848': 4, '308279': 5}\n"
     ]
    }
   ],
   "source": [
    "pic_label_dict = {}\n",
    "limit_lines = 1000\n",
    "f = open(label_file, \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    if i >= limit_lines:\n",
    "        break\n",
    "    line_array = line.split()\n",
    "#     print(line_array)\n",
    "    picture_name = line_array[1]\n",
    "    # print(picture_name)\n",
    "    temp = line_array[2:]\n",
    "    # print(temp)\n",
    "    aesthetic_values = temp[:10]\n",
    "    # print(aesthetic_values)\n",
    "    for i in range(0, len(aesthetic_values)): \n",
    "        aesthetic_values[i] = int(aesthetic_values[i])\n",
    "    # print(max(aesthetic_values))\n",
    "    pic_label_dict[picture_name] = np.asarray(aesthetic_values).argmax()\n",
    "print(pic_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "#         print(tuple_with_path)\n",
    "        return tuple_with_path\n",
    "ratings = None\n",
    "class ImageFolderWithPathsAndRatings(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPathsAndRatings, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "#         return tuple_with_path\n",
    "        # set rating\n",
    "        try:\n",
    "            rating = ratings[index] - 1 if ratings[index] > 0 else ratings[index]\n",
    "            tuple_with_path_and_rating = (tuple_with_path + (rating,))\n",
    "        except:\n",
    "            tuple_with_path_and_rating = (tuple_with_path + (torch.FloatTensor([0]),))\n",
    "        return tuple_with_path_and_rating\n",
    "\n",
    "def find_size_bounds(limit_num_pictures=None):\n",
    "    \"\"\" Will print and return min/max width/height of pictures in the dataset \n",
    "    :param limit_num_pictures - limits the number of pictures analyzed if you purposefully \n",
    "        want to work with a smaller dataset\n",
    "    \"\"\"\n",
    "    data = ImageFolderWithPaths(data_dir)\n",
    "    print(data[0][0].size)\n",
    "    max_h = (data[0][0]).size[1]\n",
    "    min_h = data[0][0].size[1]\n",
    "    max_w = data[0][0].size[0]\n",
    "    min_w = data[0][0].size[0]\n",
    "    try:\n",
    "        for (i, pic) in enumerate(data):\n",
    "            #if we are limiting pics\n",
    "            if limit_num_pictures:\n",
    "                if i > limit_num_pictures:\n",
    "                    break\n",
    "            print(pic[0].size) # print all size dimensions\n",
    "            \n",
    "            # check width records\n",
    "            if pic[0].size[0] > max_w:\n",
    "                max_w = pic[0].size[0]\n",
    "            elif pic[0].size[1] < min_w:\n",
    "                min_w = pic[0].size[0]\n",
    "\n",
    "            # check height records\n",
    "            if pic[0].size[1] > max_h:\n",
    "                max_h = pic[0].size[1]\n",
    "            elif pic[0].size[1] < min_h:\n",
    "                min_h = pic[0].size[1]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"error occurred on pic {} number {}\".format(pic, i))\n",
    "\n",
    "    print(\"Max/min width: {} {}\".format(max_w, min_w))\n",
    "    print(\"Max/min height: {} {}\".format(max_h, min_h))\n",
    "    return min_w, max_w, min_h, max_h\n",
    "#find_size_bounds(limit_num_pictures=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and apply the transforms on contained pictures\n",
    "\n",
    "_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = ImageFolderWithPaths(data_dir, transform=_transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data)#, num_workers=4)\n",
    "\n",
    "limit_num_pictures = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures in subdirectories: 255508\n",
      "Size of training set: 204407, size of test set: 51101\n"
     ]
    }
   ],
   "source": [
    "# Define our data transforms to get all our images the same size\n",
    "_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "valid_size = 0.2 # percentage of data to use for test set\n",
    "\n",
    "# load data and apply the transforms on contained pictures\n",
    "train_data = ImageFolderWithPathsAndRatings(data_dir, transform=_transform)\n",
    "test_data = ImageFolderWithPathsAndRatings(data_dir, transform=_transform)   \n",
    "\n",
    "num_pictures = len(train_data)\n",
    "print(\"Number of pictures in subdirectories: {}\".format(num_pictures))\n",
    "\n",
    "# Shuffle pictures and split training set\n",
    "indices = list(range(num_pictures))\n",
    "# print(\"Head of indices: {}\".format(indices[:10]))\n",
    "\n",
    "split = int(np.floor(valid_size * num_pictures))\n",
    "# print(\"Split index: {}\".format(split))\n",
    "\n",
    "# may be unnecessary with the choice of sampler below\n",
    "#     np.random.shuffle(indices)\n",
    "#     print(\"Head of shuffled indices: {}\".format(indices[:10]))\n",
    "\n",
    "train_idx, test_idx = indices[split:], indices[:split]#rated_indices, bad_indices\n",
    "print(\"Size of training set: {}, size of test set: {}\".format(len(train_idx), len(test_idx)))\n",
    "\n",
    "# Define samplers that sample elements randomly without replacement\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# Define data loaders, which allow batching and shuffling the data\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "               sampler=train_sampler, batch_size=1)#, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "               sampler=test_sampler, batch_size=1)#, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device that will be used: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device that will be used: {}\".format(device))\n",
    "\n",
    "# we load the pretrained model, the argument pretrained=True implies to load the ImageNet \n",
    "#     weights for the pre-trained model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.to(device) # loads the model onto the device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False #freeze all convolution weights\n",
    "network = list(vgg16.classifier.children())[:-1] #remove fully connected layer\n",
    "network.extend([nn.Linear(4096, 8)]) #add new layer of 4096->100 (rating scale with 1 decimal - similar to 1 hot encoding)\n",
    "vgg16.classifier = nn.Sequential(*network)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.4, momentum=0.9) # optimizer\n",
    "\n",
    "vgg16 #print out the model to ensure our network is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "tensor([139952145791968,       102076304,              32,             129,\n",
      "              102119184])\n",
      "tensor([[-0.9496, -0.0803,  0.2219,  0.0148, -0.4747, -0.3181,  0.3201, -0.2767]],\n",
      "       grad_fn=<AddmmBackward>) tensor([139952145791968])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-574e4c4507b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97"
     ]
    }
   ],
   "source": [
    "vgg16.train() # set model to training model\n",
    "num_epochs = 1 \n",
    "training_loss = 0\n",
    "training_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        if limit_num_pictures:\n",
    "            if i > limit_num_pictures:\n",
    "                break\n",
    "        inputs, _, path, label = data\n",
    "        path = path[0]\n",
    "#         print(path)\n",
    "        path_array = path.split('/')\n",
    "        pic_name = path_array[-1]\n",
    "        print(pic_label_dict[pic_name.split('.')[0]])\n",
    "        label = torch.LongTensor(pic_label_dict[pic_name.split('.')[0]])\n",
    "        print(label)\n",
    "        optimizer.zero_grad()\n",
    "        output = vgg16(inputs)\n",
    "        print(output, torch.LongTensor(pic_label_dict[pic_name.split('.')[0]]))\n",
    "        loss = criterion(output, torch.LongTensor(pic_label_dict[pic_name.split('.')[0]]))\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        num_correct += (preds == pic_label_dict[pic_name.split('.')]).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        print(\"Completed training output for image #{}: {}\".format(i, output))\n",
    "        if epoch == 0 and i % 20 == 0:\n",
    "            fig = plt.figure(figsize=(16, 4))\n",
    "            columns = 3\n",
    "            rows = 1\n",
    "            short_name = ''\n",
    "            short_name.join(path[0].split('/')[8:])\n",
    "            print(short_name)\n",
    "            img = mpimg.imread(path[0])\n",
    "            fig.add_subplot(rows, columns, 1)\n",
    "            plt.imshow(img)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.show()\n",
    "    training_loss = running_loss/len(train_loader.dataset)\n",
    "    training_accuracy = 100 * num_correct/len(train_loader.dataset)\n",
    "    print(\"Training accuracy: {}, Training loss: {}\".format(training_accuracy, training_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit_num_pictures = 20\n",
    "vgg16.eval() # set to prediction mode\n",
    "testing_loss = 0\n",
    "testing_accuracy = 0\n",
    "running_loss = 0.0\n",
    "num_correct = 0\n",
    "\n",
    "ratings_data = None\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    \n",
    "    if limit_num_pictures:\n",
    "        if i > limit_num_pictures:\n",
    "            break\n",
    "    inputs, _, path, label = data\n",
    "    path = path[0]\n",
    "    label = torch.LongTensor([int(label[0])])\n",
    "\n",
    "    output = vgg16(inputs)\n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    _, preds = torch.max(output.data, 1)\n",
    "    num_correct += (preds == label).sum().item()\n",
    "    ratings = output[0].tolist()\n",
    "    print(\"\\nImage path: {}\".format(path))\n",
    "    print(\"Classification for test image #{}: {}\".format(i, ratings))\n",
    "    tuple_to_insert = {}\n",
    "    for n in range(8):\n",
    "        tuple_to_insert[str(n + 1)] = [ratings[n]]\n",
    "    tuple_to_insert['file_path'] = [path]\n",
    "    tuple_to_insert = pandas.DataFrame.from_dict(tuple_to_insert)\n",
    "\n",
    "    if i == 0:\n",
    "        ratings_data = tuple_to_insert\n",
    "    else:\n",
    "        ratings_data = ratings_data.append(tuple_to_insert, ignore_index=True)\n",
    "    print(ratings_data.tail(0))\n",
    "    if i % 2000 == 0:\n",
    "        fig = plt.figure(figsize=(16, 4))\n",
    "        columns = 3\n",
    "        rows = 1\n",
    "        img = mpimg.imread(path)\n",
    "        fig.add_subplot(rows, columns, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "testing_loss = running_loss/len(test_loader.dataset)\n",
    "testing_accuracy = 100. * num_correct/len(test_loader.dataset)\n",
    "ratings_data = ratings_data.set_index('file_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "ratings_data_norm = pd.DataFrame(scaler.fit_transform(ratings_data), columns=ratings_data.columns, index=ratings_data.index)\n",
    "ratings_data_norm.hist()\n",
    "ratings_data.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data_norm['classification'] = ratings_data_norm.idxmax(axis=1)\n",
    "ratings_data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data_norm = ratings_data_norm.sort_values(by=['classification'])\n",
    "ratings_data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_images = ratings_data_norm.loc[ratings_data_norm['classification'] == '8']\n",
    "good_images = ratings_data_norm.loc[ratings_data_norm['classification'] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "for path in good_images.head().index:\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    img = mpimg.imread(path)\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(16, 4))\n",
    "for path in bad_images.head().index:\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    img = mpimg.imread(path)\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
