{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard ML/Image Processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, pandas\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#pytorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# our custom classes for loading images with paths and/or ratings\n",
    "from image_classification_file import ImageFolderWithPathsAndRatings, ImageFolderWithPaths\n",
    "\n",
    "# no one likes irrelevant warnings\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# root directory where the images are stored\n",
    "data_dir = \"/mnt/md0/reynolds/ava-dataset/\"\n",
    "label_file = \"/mnt/md0/reynolds/ava-dataset/AVA_dataset/AVA.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pic_label_dict = {}\n",
    "limit_lines = None\n",
    "f = open(label_file, \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    if limit_lines:\n",
    "        if i >= limit_lines:\n",
    "            break\n",
    "    line_array = line.split()\n",
    "#     print(line_array)\n",
    "    picture_name = line_array[1]\n",
    "    # print(picture_name)\n",
    "    temp = line_array[2:]\n",
    "    # print(temp)\n",
    "    aesthetic_values = temp[:10]\n",
    "    # print(aesthetic_values)\n",
    "    for i in range(0, len(aesthetic_values)): \n",
    "        aesthetic_values[i] = int(aesthetic_values[i])\n",
    "    # print(max(aesthetic_values))\n",
    "    pic_label_dict[picture_name] = np.asarray(aesthetic_values).argmax()\n",
    "print(pic_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and apply the transforms on contained pictures\n",
    "\n",
    "_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = ImageFolderWithPaths(data_dir, transform=_transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data)#, num_workers=4)\n",
    "\n",
    "limit_num_pictures = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures in subdirectories: 255508\n",
      "Size of training set: 204407, size of test set: 51101\n"
     ]
    }
   ],
   "source": [
    "# Define our data transforms to get all our images the same size\n",
    "_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "valid_size = 0.2 # percentage of data to use for test set\n",
    "\n",
    "# load data and apply the transforms on contained pictures\n",
    "train_data = ImageFolderWithPathsAndRatings(data_dir, transform=_transform)\n",
    "test_data = ImageFolderWithPathsAndRatings(data_dir, transform=_transform)   \n",
    "\n",
    "num_pictures = len(train_data)\n",
    "print(\"Number of pictures in subdirectories: {}\".format(num_pictures))\n",
    "\n",
    "# Shuffle pictures and split training set\n",
    "indices = list(range(num_pictures))\n",
    "# print(\"Head of indices: {}\".format(indices[:10]))\n",
    "\n",
    "split = int(np.floor(valid_size * num_pictures))\n",
    "# print(\"Split index: {}\".format(split))\n",
    "\n",
    "# may be unnecessary with the choice of sampler below\n",
    "#     np.random.shuffle(indices)\n",
    "#     print(\"Head of shuffled indices: {}\".format(indices[:10]))\n",
    "\n",
    "train_idx, test_idx = indices[split:], indices[:split]#rated_indices, bad_indices\n",
    "print(\"Size of training set: {}, size of test set: {}\".format(len(train_idx), len(test_idx)))\n",
    "\n",
    "# Define samplers that sample elements randomly without replacement\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# Define data loaders, which allow batching and shuffling the data\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "               sampler=train_sampler, batch_size=1)#, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "               sampler=test_sampler, batch_size=1)#, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device that will be used: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device that will be used: {}\".format(device))\n",
    "\n",
    "# we load the pretrained model, the argument pretrained=True implies to load the ImageNet \n",
    "#     weights for the pre-trained model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.to(device) # loads the model onto the device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False #freeze all convolution weights\n",
    "network = list(vgg16.classifier.children())[:-1] #remove fully connected layer\n",
    "network.extend([nn.Linear(4096, 8)]) #add new layer of 4096->100 (rating scale with 1 decimal - similar to 1 hot encoding)\n",
    "vgg16.classifier = nn.Sequential(*network)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.4, momentum=0.9) # optimizer\n",
    "\n",
    "vgg16 #print out the model to ensure our network is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'658705'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8672ef0e5dea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpath_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic_label_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic_label_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '658705'"
     ]
    }
   ],
   "source": [
    "vgg16.train() # set model to training model\n",
    "num_epochs = 1 \n",
    "training_loss = 0\n",
    "training_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        if limit_num_pictures:\n",
    "            if i > limit_num_pictures:\n",
    "                break\n",
    "        inputs, _, path, label = data\n",
    "        path = path[0]\n",
    "#         print(path)\n",
    "        path_array = path.split('/')\n",
    "        pic_name = path_array[-1]\n",
    "        print(pic_label_dict[pic_name.split('.')[0]])\n",
    "        label = torch.LongTensor(pic_label_dict[pic_name.split('.')[0]])\n",
    "        print(label)\n",
    "        optimizer.zero_grad()\n",
    "        output = vgg16(inputs)\n",
    "        print(output, torch.LongTensor(pic_label_dict[pic_name.split('.')[0]]))\n",
    "        loss = criterion(output, torch.LongTensor(pic_label_dict[pic_name.split('.')[0]]))\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        num_correct += (preds == pic_label_dict[pic_name.split('.')]).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        print(\"Completed training output for image #{}: {}\".format(i, output))\n",
    "        if epoch == 0 and i % 20 == 0:\n",
    "            fig = plt.figure(figsize=(16, 4))\n",
    "            columns = 3\n",
    "            rows = 1\n",
    "            short_name = ''\n",
    "            short_name.join(path[0].split('/')[8:])\n",
    "            print(short_name)\n",
    "            img = mpimg.imread(path[0])\n",
    "            fig.add_subplot(rows, columns, 1)\n",
    "            plt.imshow(img)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.show()\n",
    "    training_loss = running_loss/len(train_loader.dataset)\n",
    "    training_accuracy = 100 * num_correct/len(train_loader.dataset)\n",
    "    print(\"Training accuracy: {}, Training loss: {}\".format(training_accuracy, training_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit_num_pictures = 20\n",
    "vgg16.eval() # set to prediction mode\n",
    "testing_loss = 0\n",
    "testing_accuracy = 0\n",
    "running_loss = 0.0\n",
    "num_correct = 0\n",
    "\n",
    "ratings_data = None\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    \n",
    "    if limit_num_pictures:\n",
    "        if i > limit_num_pictures:\n",
    "            break\n",
    "    inputs, _, path, label = data\n",
    "    path = path[0]\n",
    "    label = torch.LongTensor([int(label[0])])\n",
    "\n",
    "    output = vgg16(inputs)\n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    _, preds = torch.max(output.data, 1)\n",
    "    num_correct += (preds == label).sum().item()\n",
    "    ratings = output[0].tolist()\n",
    "    print(\"\\nImage path: {}\".format(path))\n",
    "    print(\"Classification for test image #{}: {}\".format(i, ratings))\n",
    "    tuple_to_insert = {}\n",
    "    for n in range(8):\n",
    "        tuple_to_insert[str(n + 1)] = [ratings[n]]\n",
    "    tuple_to_insert['file_path'] = [path]\n",
    "    tuple_to_insert = pandas.DataFrame.from_dict(tuple_to_insert)\n",
    "\n",
    "    if i == 0:\n",
    "        ratings_data = tuple_to_insert\n",
    "    else:\n",
    "        ratings_data = ratings_data.append(tuple_to_insert, ignore_index=True)\n",
    "    print(ratings_data.tail(0))\n",
    "    if i % 2000 == 0:\n",
    "        fig = plt.figure(figsize=(16, 4))\n",
    "        columns = 3\n",
    "        rows = 1\n",
    "        img = mpimg.imread(path)\n",
    "        fig.add_subplot(rows, columns, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "testing_loss = running_loss/len(test_loader.dataset)\n",
    "testing_accuracy = 100. * num_correct/len(test_loader.dataset)\n",
    "ratings_data = ratings_data.set_index('file_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "ratings_data_norm = pd.DataFrame(scaler.fit_transform(ratings_data), columns=ratings_data.columns, index=ratings_data.index)\n",
    "ratings_data_norm.hist()\n",
    "ratings_data.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data_norm['classification'] = ratings_data_norm.idxmax(axis=1)\n",
    "ratings_data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data_norm = ratings_data_norm.sort_values(by=['classification'])\n",
    "ratings_data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_images = ratings_data_norm.loc[ratings_data_norm['classification'] == '8']\n",
    "good_images = ratings_data_norm.loc[ratings_data_norm['classification'] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "for path in good_images.head().index:\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    img = mpimg.imread(path)\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(16, 4))\n",
    "for path in bad_images.head().index:\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    img = mpimg.imread(path)\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
