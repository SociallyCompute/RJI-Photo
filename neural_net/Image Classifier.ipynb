{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Images into Subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we shall attempt to classify images into subgroups. This is intended to help rank the images more effectively against other similar images. This would be added to the front of any ranking pipeline. Because of the labels provided we are only able to train with the AVA dataset (255510 images total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SCRIPT IMPORTS\n",
    "'''\n",
    "#standard ML/Image Processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, pandas\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#pytorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# no one likes irrelevant warnings\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# we load the pretrained model, the argument pretrained=True implies to load the ImageNet weights for the pre-trained model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# root directory where the images are stored\n",
    "data_dir = \"/mnt/md0/reynolds/ava-dataset/\"\n",
    "label_file = \"/mnt/md0/reynolds/ava-dataset/AVA_dataset/AVA.txt\"\n",
    "tags_file = \"/mnt/md0/reynolds/ava-dataset/AVA_dataset/tags.txt\"\n",
    "limit_lines = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Abstract', 24: 'Action', 31: 'Advertisement', 66: 'Analog', 19: 'Animals', 20: 'Architecture', 43: 'Astrophotography', 57: 'Birds', 21: 'Black', 51: 'Blur', 64: 'Camera', 16: 'Candid', 50: 'Children', 2: 'Cityscape', 34: 'Digital', 37: 'Diptych', 49: 'DPChallenge', 12: 'Emotive', 4: 'Family', 3: 'Fashion', 63: 'Fish', 38: 'Floral', 40: 'Food', 53: 'High', 45: 'History', 58: 'Horror', 5: 'Humorous', 46: 'Infrared', 65: 'Insects,', 6: 'Interior', 14: 'Landscape', 62: 'Lensbaby', 22: 'Macro', 56: 'Maternity', 44: 'Military', 59: 'Music', 15: 'Nature', 26: 'Nude', 55: 'Overlays', 33: 'Panoramic', 13: 'Performance', 32: 'Persuasive', 52: 'Photo-Impressionism', 25: 'Photojournalism', 60: 'Pinhole/Zone', 30: 'Political', 17: 'Portraiture', 27: 'Rural', 41: 'Science', 35: 'Seascapes', 47: 'Self', 7: 'Sky', 8: 'Snapshot', 9: 'Sports', 18: 'Still', 61: 'Street', 29: 'Studio', 54: 'Texture', 48: 'Textures', 36: 'Traditional', 39: 'Transportation', 23: 'Travel', 10: 'Urban', 11: 'Vintage', 28: 'Water', 42: 'Wedding', 0: 'Miscellaneous'}\n"
     ]
    }
   ],
   "source": [
    "tag_mapping = {}\n",
    "f = open(tags_file, \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    if i >= limit_lines:\n",
    "        break\n",
    "    line_array = line.split()\n",
    "    tag_mapping[int(line_array[0])] = line_array[1]\n",
    "tag_mapping[0] = \"Miscellaneous\"\n",
    "print(tag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_label_dict = {}\n",
    "f = open(label_file, \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    if i >= limit_lines:\n",
    "        break\n",
    "    line_array = line.split()\n",
    "#     print(line_array)\n",
    "    picture_name = line_array[1]\n",
    "    # print(picture_name)\n",
    "    classifications = (line_array[12:])[:-1]\n",
    "#     print(classifications)\n",
    "    for i in range(0, len(classifications)): \n",
    "#         classifications[i] = tag_mapping[int(classifications[i])]\n",
    "        classifications[i] = int(classifications[i])\n",
    "    # print(max(classifications))\n",
    "    pic_label_dict[picture_name] = classifications\n",
    "# print(pic_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "#         print(tuple_with_path)\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-208c6633d757>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-208c6633d757>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    def _get_target(self, file_path):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Replace the labels in the Dataloaders\n",
    "class AvaDataset(Dataset):\n",
    "    def __init__(self, image_path, transform=_transform):\n",
    "        super(AvaDataset, self).__init__()\n",
    "        #TODO\n",
    "    def _find_classes(self, dir):\n",
    "        #TODO\n",
    "    def _get_target(self, file_path):\n",
    "        #TODO\n",
    "    def make_dataset(self, dir, class_to_idx):\n",
    "        #TODO\n",
    "    def get_class_dict(self):\n",
    "        #TODO\n",
    "    def __getitem__(self, index):\n",
    "        #TODO\n",
    "    def __len__(self):\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Define our data transforms to get all our images the same size\n",
    "_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "data = ImageFolderWithPaths(data_dir, transform=_transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data)#, num_workers=4)\n",
    "\n",
    "limit_num_pictures = 1000000\n",
    "\n",
    "valid_size = 0.2 # percentage of data to use for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = data\n",
    "train_data = data\n",
    "num_pictures = len(train_data)\n",
    "\n",
    "# Shuffle pictures and split training set\n",
    "indices = list(range(num_pictures))\n",
    "\n",
    "split = int(np.floor(valid_size * num_pictures))\n",
    "train_idx, test_idx = indices[split:], indices[:split]#rated_indices, bad_indices\n",
    "#print(\"Size of training set: {}, size of test set: {}\".format(len(train_idx), len(test_idx)))\n",
    "\n",
    "# Define samplers that sample elements randomly without replacement\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# Define data loaders, which allow batching and shuffling the data\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "               sampler=train_sampler, batch_size=30)#, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "               sampler=test_sampler, batch_size=30)#, num_workers=4)\n",
    "\n",
    "# check GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# we load the pretrained model, the argument pretrained=True implies to load the ImageNet \n",
    "#     weights for the pre-trained model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.to(device) # loads the model onto the device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False #freeze all convolution weights\n",
    "network = list(vgg16.classifier.children())[:-1] #remove fully connected layer\n",
    "network.extend([nn.Linear(4096, 67)]) #add new layer of 4096->100 (rating scale with 1 decimal - similar to 1 hot encoding)\n",
    "vgg16.classifier = nn.Sequential(*network)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.4, momentum=0.9) # optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.train() # set model to training model\n",
    "num_epochs = 3 \n",
    "training_loss = 0\n",
    "training_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    print(train_loader.dataset[0])\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        #print(i)\n",
    "        for j in range(2): #done so we can utilize both labels\n",
    "            if limit_num_pictures:\n",
    "                if i > limit_num_pictures:\n",
    "                    break\n",
    "            inputs, _, path = data\n",
    "            path = path[0]\n",
    "            path_array = path.split('/')\n",
    "            pic_name = path_array[-1]\n",
    "    #         print(pic_name)\n",
    "    #         print(pic_label_dict[pic_name.split('.')[0]])\n",
    "    #         label = torch.LongTensor(pic_label_dict[pic_name.split('.')[0]])\n",
    "            label = pic_label_dict[pic_name.split('.')[0]][j]\n",
    "#             print(tag_mapping[label])\n",
    "            label = torch.LongTensor([label])\n",
    "            print('inputs shape is: {}'.format(inputs.shape))\n",
    "            print('label shape is: {}'.format(label.shape))\n",
    "            print('label is : {}'.format(label))\n",
    "            optimizer.zero_grad()\n",
    "            output = vgg16(inputs)\n",
    "            print('output shape is: {} for image #{}'.format(output.shape, i))\n",
    "    #         print(output, label)\n",
    "            loss = criterion(output, label)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            num_correct += (preds == pic_label_dict[pic_name.split('.')[0]][j]).sum().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    training_loss = running_loss/len(train_loader.dataset)\n",
    "    training_accuracy = 100 * num_correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg16.state_dict(), 'models/CLASSIFICATION_Feb3_All_AVA_only_training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.load_state_dict(torch.load('models/CLASSIFICATION_Feb18_All_AVA_only_training.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[ 1.6838,  1.6667,  1.6324,  ...,  1.9407,  1.9064,  1.8722],\n",
      "          [ 1.6495,  1.6495,  1.6495,  ...,  1.9235,  1.8722,  1.8037],\n",
      "          [ 1.6495,  1.6495,  1.6495,  ...,  1.9235,  1.8722,  1.8550],\n",
      "          ...,\n",
      "          [ 1.8208,  1.8208,  1.8208,  ...,  0.3652, -0.2342, -0.6623],\n",
      "          [ 1.8037,  1.8037,  1.7865,  ...,  0.2624, -0.3541, -0.9877],\n",
      "          [ 1.7523,  1.7352,  1.7009,  ...,  0.2453, -0.2171, -0.9534]],\n",
      "\n",
      "         [[ 1.5357,  1.5357,  1.5007,  ...,  1.5182,  1.4832,  1.4307],\n",
      "          [ 1.5007,  1.5182,  1.5182,  ...,  1.5007,  1.4482,  1.3957],\n",
      "          [ 1.5182,  1.5182,  1.5182,  ...,  1.4832,  1.4482,  1.4657],\n",
      "          ...,\n",
      "          [ 1.7633,  1.7808,  1.7458,  ..., -0.0574, -0.6702, -1.0553],\n",
      "          [ 1.7458,  1.7283,  1.6933,  ..., -0.1625, -0.7402, -1.3880],\n",
      "          [ 1.6583,  1.6583,  1.6057,  ..., -0.1975, -0.6176, -1.3704]],\n",
      "\n",
      "         [[ 1.3154,  1.2457,  1.1934,  ...,  0.9494,  0.8971,  0.8448],\n",
      "          [ 1.2805,  1.2457,  1.2108,  ...,  0.9145,  0.7925,  0.7054],\n",
      "          [ 1.2805,  1.2631,  1.2282,  ...,  0.8971,  0.8448,  0.8448],\n",
      "          ...,\n",
      "          [ 1.6814,  1.6988,  1.5942,  ..., -1.1944, -1.3339, -1.5081],\n",
      "          [ 1.5942,  1.6117,  1.5071,  ..., -1.2293, -1.4210, -1.5779],\n",
      "          [ 1.4548,  1.4374,  1.3328,  ..., -1.4036, -1.4559, -1.5779]]],\n",
      "\n",
      "\n",
      "        [[[-0.6109,  0.1768,  0.0912,  ..., -1.5357, -1.5699, -1.5699],\n",
      "          [-1.5185, -0.0458,  0.1254,  ..., -1.4329, -1.4672, -1.5014],\n",
      "          [-1.8268, -0.8507,  0.2796,  ..., -1.3130, -1.3644, -1.3987],\n",
      "          ...,\n",
      "          [ 0.3823,  0.7591,  0.8618,  ...,  0.6392,  0.4851,  0.4508],\n",
      "          [ 0.7077,  1.2728,  1.3070,  ...,  0.3138,  0.5707,  0.5536],\n",
      "          [ 0.5193,  0.7077,  1.1872,  ...,  0.4337,  0.1939,  1.0502]],\n",
      "\n",
      "         [[-0.6176,  0.1702,  0.1527,  ..., -0.4951, -0.4776, -0.5126],\n",
      "          [-1.5630, -0.0924,  0.1877,  ..., -0.4951, -0.4776, -0.5126],\n",
      "          [-1.9132, -0.9153,  0.3452,  ..., -0.5651, -0.5476, -0.5301],\n",
      "          ...,\n",
      "          [ 0.3452,  0.6779,  0.8529,  ...,  0.6429,  0.4153,  0.3803],\n",
      "          [ 0.4853,  1.0280,  1.2381,  ...,  0.3277,  0.5378,  0.5028],\n",
      "          [ 0.2402,  0.3102,  0.9055,  ...,  0.4328,  0.1527,  0.9580]],\n",
      "\n",
      "         [[-0.4450,  0.3568,  0.3219,  ...,  0.7576,  0.7402,  0.7054],\n",
      "          [-1.3861,  0.0779,  0.3568,  ...,  0.7751,  0.7576,  0.7054],\n",
      "          [-1.7173, -0.7413,  0.5136,  ...,  0.7925,  0.7576,  0.7402],\n",
      "          ...,\n",
      "          [ 0.1476,  0.6182,  0.8622,  ...,  0.6531,  0.4265,  0.3219],\n",
      "          [ 0.3393,  0.9842,  1.2980,  ...,  0.3568,  0.5834,  0.4962],\n",
      "          [ 0.2348,  0.2696,  0.9319,  ...,  0.4962,  0.1999,  0.9319]]],\n",
      "\n",
      "\n",
      "        [[[-0.9363, -1.2788, -1.4500,  ..., -1.6042, -1.4329, -0.9363],\n",
      "          [-1.1418, -1.7240, -1.1418,  ..., -1.6213, -1.2103, -0.8849],\n",
      "          [-1.4329, -1.5357, -0.9705,  ..., -1.4158, -0.9705, -0.8849],\n",
      "          ...,\n",
      "          [-0.3541, -0.6281,  0.4337,  ..., -0.4911,  0.4679,  0.7591],\n",
      "          [-0.6623,  0.5022,  0.3481,  ..., -0.9705, -1.0048, -1.2445],\n",
      "          [-0.2342,  0.6049,  0.0398,  ..., -1.1932, -1.2617, -1.3130]],\n",
      "\n",
      "         [[-0.0749, -0.6352, -0.9678,  ..., -1.1078, -0.8102, -0.1450],\n",
      "          [-0.3200, -1.2129, -0.5476,  ..., -1.1078, -0.5301, -0.0749],\n",
      "          [-0.8102, -1.0378, -0.2150,  ..., -0.8978, -0.2150, -0.0574],\n",
      "          ...,\n",
      "          [ 0.4328, -0.1099,  1.0105,  ..., -0.1800,  0.6954,  0.9755],\n",
      "          [ 0.0476,  0.9930,  1.0280,  ..., -0.4601, -0.6001, -0.8627],\n",
      "          [ 0.3978,  1.1506,  0.8529,  ..., -0.5301, -0.7052, -0.7752]],\n",
      "\n",
      "         [[-1.4733, -1.4907, -1.6650,  ..., -1.7347, -1.6476, -1.5604],\n",
      "          [-1.6650, -1.7522, -1.7173,  ..., -1.7522, -1.6650, -1.6302],\n",
      "          [-1.6824, -1.6476, -1.6999,  ..., -1.6999, -1.6127, -1.6476],\n",
      "          ...,\n",
      "          [-1.3687, -1.3164, -1.0550,  ..., -0.8458,  0.3568,  0.8274],\n",
      "          [-1.4210, -0.8807, -1.1421,  ..., -1.3861, -1.2990, -1.4559],\n",
      "          [-1.2641, -1.0550, -1.3339,  ..., -1.7347, -1.7347, -1.6476]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2856, -0.3198, -0.4226,  ..., -0.4739, -0.3369, -0.2171],\n",
      "          [-0.3883, -0.4739, -0.4568,  ..., -0.3883, -0.2684, -0.1314],\n",
      "          [-0.4054, -0.4739, -0.4397,  ..., -0.2684, -0.1486, -0.0116],\n",
      "          ...,\n",
      "          [-1.2274, -1.2103, -1.1760,  ...,  1.2214,  1.2043,  1.1872],\n",
      "          [-1.4500, -1.3987, -1.3130,  ...,  1.1187,  1.1700,  1.1358],\n",
      "          [-1.4158, -1.3302, -1.2959,  ...,  1.0331,  1.1015,  1.2385]],\n",
      "\n",
      "         [[-0.6702, -0.6877, -0.7577,  ..., -0.9853, -0.8277, -0.6527],\n",
      "          [-0.8452, -0.8102, -0.8452,  ..., -0.8978, -0.7052, -0.5651],\n",
      "          [-0.8978, -0.8452, -0.8277,  ..., -0.7927, -0.6176, -0.4776],\n",
      "          ...,\n",
      "          [-1.7731, -1.7381, -1.7206,  ...,  1.2031,  1.1681,  1.1506],\n",
      "          [-1.8081, -1.7731, -1.7206,  ...,  1.1155,  1.2206,  1.2031],\n",
      "          [-1.7906, -1.7556, -1.7206,  ...,  1.0455,  1.1331,  1.2731]],\n",
      "\n",
      "         [[-1.6999, -1.6650, -1.6302,  ..., -1.7696, -1.7870, -1.7347],\n",
      "          [-1.7522, -1.7696, -1.6999,  ..., -1.7870, -1.7522, -1.6824],\n",
      "          [-1.7870, -1.8044, -1.7347,  ..., -1.8044, -1.7347, -1.6476],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ...,  1.0888,  1.0365,  1.1062],\n",
      "          [-1.8044, -1.8044, -1.7870,  ...,  1.0539,  1.1237,  1.1934],\n",
      "          [-1.8044, -1.7870, -1.7696,  ...,  0.9842,  1.0191,  1.2282]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0092,  2.0434,  2.0434,  ..., -1.2103, -1.2274, -1.3130],\n",
      "          [ 1.9920,  2.0434,  2.0605,  ..., -1.2959, -1.3815, -1.1247],\n",
      "          [ 2.0092,  2.0777,  2.0948,  ..., -1.2445, -1.1760, -0.9705],\n",
      "          ...,\n",
      "          [ 1.7180,  1.6667,  1.5810,  ..., -1.2274, -1.2788, -1.1247],\n",
      "          [ 1.7523,  1.6495,  1.6667,  ..., -1.1589, -1.0733, -1.0733],\n",
      "          [ 1.7180,  1.6495,  1.6495,  ..., -1.2274, -1.3302, -1.2103]],\n",
      "\n",
      "         [[ 1.5707,  1.6933,  1.6933,  ..., -1.4055, -1.3179, -1.2654],\n",
      "          [ 1.5357,  1.6408,  1.6933,  ..., -1.3179, -1.2479, -1.3004],\n",
      "          [ 1.5707,  1.6583,  1.6408,  ..., -1.2654, -1.2304, -1.1954],\n",
      "          ...,\n",
      "          [ 1.4832,  1.6057,  1.4307,  ..., -1.2829, -1.2479, -1.2304],\n",
      "          [ 1.6758,  1.4482,  1.3606,  ..., -1.3004, -1.3179, -1.3179],\n",
      "          [ 1.5532,  1.3782,  1.3256,  ..., -1.3354, -1.3004, -1.2829]],\n",
      "\n",
      "         [[ 1.3677,  1.4374,  1.4722,  ..., -1.6999, -1.6650, -1.6824],\n",
      "          [ 1.2805,  1.4548,  1.4722,  ..., -1.6824, -1.6302, -1.5779],\n",
      "          [ 1.3502,  1.4897,  1.4548,  ..., -1.6999, -1.6650, -1.5604],\n",
      "          ...,\n",
      "          [ 2.0997,  2.0997,  2.0823,  ..., -1.6824, -1.6650, -1.5953],\n",
      "          [ 2.1694,  2.1171,  2.0823,  ..., -1.6999, -1.6650, -1.6127],\n",
      "          [ 2.1346,  2.0300,  2.0300,  ..., -1.6999, -1.7347, -1.6302]]]]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1]), ('/mnt/md0/reynolds/ava-dataset/images/251665.jpg', '/mnt/md0/reynolds/ava-dataset/images/26339.jpg', '/mnt/md0/reynolds/ava-dataset/images/193153.jpg', '/mnt/md0/reynolds/ava-dataset/images/105615.jpg', '/mnt/md0/reynolds/ava-dataset/images/101652.jpg', '/mnt/md0/reynolds/ava-dataset/images/149185.jpg', '/mnt/md0/reynolds/ava-dataset/images/245850.jpg', '/mnt/md0/reynolds/ava-dataset/images/11284.jpg', '/mnt/md0/reynolds/ava-dataset/images/222942.jpg', '/mnt/md0/reynolds/ava-dataset/images/150733.jpg', '/mnt/md0/reynolds/ava-dataset/images/108363.jpg', '/mnt/md0/reynolds/ava-dataset/images/176514.jpg', '/mnt/md0/reynolds/ava-dataset/images/161146.jpg', '/mnt/md0/reynolds/ava-dataset/images/200033.jpg', '/mnt/md0/reynolds/ava-dataset/images/207774.jpg', '/mnt/md0/reynolds/ava-dataset/images/118127.jpg', '/mnt/md0/reynolds/ava-dataset/images/141188.jpg', '/mnt/md0/reynolds/ava-dataset/images/176144.jpg', '/mnt/md0/reynolds/ava-dataset/images/105010.jpg', '/mnt/md0/reynolds/ava-dataset/images/264921.jpg', '/mnt/md0/reynolds/ava-dataset/images/181188.jpg', '/mnt/md0/reynolds/ava-dataset/images/121122.jpg', '/mnt/md0/reynolds/ava-dataset/images/224578.jpg', '/mnt/md0/reynolds/ava-dataset/images/161777.jpg', '/mnt/md0/reynolds/ava-dataset/images/224888.jpg', '/mnt/md0/reynolds/ava-dataset/images/22685.jpg', '/mnt/md0/reynolds/ava-dataset/images/111842.jpg', '/mnt/md0/reynolds/ava-dataset/images/1585.jpg', '/mnt/md0/reynolds/ava-dataset/images/100601.jpg', '/mnt/md0/reynolds/ava-dataset/images/180461.jpg')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image path: 1\n",
      "Classification for test image #0: [6808.9931640625, -4866.26953125, -5490.67822265625, -3223.6474609375, -6502.66162109375, 6734.462890625, 2561.2998046875, 1292.581298828125, -1667.1268310546875, -5692.37451171875, -3387.877685546875, -2045.61328125, 5637.71875, -5997.080078125, -1154.7723388671875, -1235.27978515625, 541.7202758789062, -3926.94580078125, 268.7127685546875, 15281.6298828125, 2157.350341796875, -1525.4141845703125, -5233.68701171875, -1709.6614990234375, -8213.2138671875, -2562.50537109375, -2265.681396484375, 7602.5625, -1330.9215087890625, -3348.30810546875, 2198.16845703125, 5064.84326171875, 3243.326416015625, 2315.56201171875, 1275.9124755859375, -4073.347412109375, -3135.10400390625, 3447.738037109375, -9936.185546875, 5898.71630859375, 7184.99560546875, -3628.92333984375, -1617.277587890625, -3416.5791015625, 65.30950927734375, -2788.5830078125, 1728.1512451171875, 4339.82861328125, 4059.336181640625, 1937.939453125, -3179.497802734375, -6016.59375, 4750.04736328125, 2384.11962890625, 1822.6697998046875, 1607.352294921875, 173.3772735595703, -8080.7265625, 1045.1287841796875, -4867.86767578125, 2768.249267578125, 6894.708984375, 4510.0595703125, 1535.7125244140625, 3896.197998046875, -884.7333984375, -12.466757774353027]\n",
      "Empty DataFrame\n",
      "Columns: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, file_path]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-94dbae0e9022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1401\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m             \u001b[0mbasename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/posixpath.py\u001b[0m in \u001b[0;36msplitext\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "limit_num_pictures = 2500\n",
    "vgg16.eval() # set to prediction mode\n",
    "testing_loss = 0\n",
    "testing_accuracy = 0\n",
    "running_loss = 0.0\n",
    "num_correct = 0\n",
    "\n",
    "ratings_data = None\n",
    "for data in test_loader:\n",
    "    print(data)\n",
    "    \n",
    "    if limit_num_pictures:\n",
    "        if i > limit_num_pictures:\n",
    "            break\n",
    "    inputs, path, _ = data\n",
    "    path = path[0]\n",
    "#     label = torch.LongTensor([int(label[0])])\n",
    "\n",
    "    output = vgg16(inputs)\n",
    "#     loss = criterion(output, label)\n",
    "\n",
    "#     running_loss += loss.item()\n",
    "    _, preds = torch.max(output.data, 1)\n",
    "#     num_correct += (preds == label).sum().item()\n",
    "    ratings = output[0].tolist()\n",
    "    print(\"\\nImage path: {}\".format(path))\n",
    "    print(\"Classification for test image #{}: {}\".format(i, ratings))\n",
    "    tuple_to_insert = {}\n",
    "    for n in range(10):\n",
    "        tuple_to_insert[str(n + 1)] = [ratings[n]]\n",
    "    tuple_to_insert['file_path'] = [path]\n",
    "    tuple_to_insert = pandas.DataFrame.from_dict(tuple_to_insert)\n",
    "\n",
    "    if i == 0:\n",
    "        ratings_data = tuple_to_insert\n",
    "    else:\n",
    "        ratings_data = ratings_data.append(tuple_to_insert, ignore_index=True)\n",
    "    print(ratings_data.tail(0))\n",
    "    if i % 200 == 0:\n",
    "        fig = plt.figure(figsize=(16, 4))\n",
    "        columns = 3\n",
    "        rows = 1\n",
    "        img = mpimg.imread(path)\n",
    "        fig.add_subplot(rows, columns, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "# testing_loss = running_loss/len(test_loader.dataset)\n",
    "# testing_accuracy = 100. * num_correct/len(test_loader.dataset)\n",
    "ratings_data = ratings_data.set_index('file_path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
