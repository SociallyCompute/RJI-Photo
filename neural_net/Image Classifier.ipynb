{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Images into Subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we shall attempt to classify images into subgroups. This is intended to help rank the images more effectively against other similar images. This would be added to the front of any ranking pipeline. Because of the labels provided we are only able to train with the AVA dataset (255510 images total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SCRIPT IMPORTS\n",
    "'''\n",
    "#standard ML/Image Processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, pandas\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#pytorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# no one likes irrelevant warnings\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# we load the pretrained model, the argument pretrained=True implies to load the ImageNet weights for the pre-trained model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# root directory where the images are stored\n",
    "data_dir = \"/mnt/md0/reynolds/ava-dataset/\"\n",
    "label_file = \"/mnt/md0/reynolds/ava-dataset/AVA_dataset/AVA.txt\"\n",
    "tags_file = \"/mnt/md0/reynolds/ava-dataset/AVA_dataset/tags.txt\"\n",
    "limit_lines = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Abstract', 24: 'Action', 31: 'Advertisement', 66: 'Analog', 19: 'Animals', 20: 'Architecture', 43: 'Astrophotography', 57: 'Birds', 21: 'Black', 51: 'Blur', 64: 'Camera', 16: 'Candid', 50: 'Children', 2: 'Cityscape', 34: 'Digital', 37: 'Diptych', 49: 'DPChallenge', 12: 'Emotive', 4: 'Family', 3: 'Fashion', 63: 'Fish', 38: 'Floral', 40: 'Food', 53: 'High', 45: 'History', 58: 'Horror', 5: 'Humorous', 46: 'Infrared', 65: 'Insects,', 6: 'Interior', 14: 'Landscape', 62: 'Lensbaby', 22: 'Macro', 56: 'Maternity', 44: 'Military', 59: 'Music', 15: 'Nature', 26: 'Nude', 55: 'Overlays', 33: 'Panoramic', 13: 'Performance', 32: 'Persuasive', 52: 'Photo-Impressionism', 25: 'Photojournalism', 60: 'Pinhole/Zone', 30: 'Political', 17: 'Portraiture', 27: 'Rural', 41: 'Science', 35: 'Seascapes', 47: 'Self', 7: 'Sky', 8: 'Snapshot', 9: 'Sports', 18: 'Still', 61: 'Street', 29: 'Studio', 54: 'Texture', 48: 'Textures', 36: 'Traditional', 39: 'Transportation', 23: 'Travel', 10: 'Urban', 11: 'Vintage', 28: 'Water', 42: 'Wedding', 0: 'Miscellaneous'}\n"
     ]
    }
   ],
   "source": [
    "tag_mapping = {}\n",
    "f = open(tags_file, \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    if i >= limit_lines:\n",
    "        break\n",
    "    line_array = line.split()\n",
    "    tag_mapping[int(line_array[0])] = line_array[1]\n",
    "tag_mapping[0] = \"Miscellaneous\"\n",
    "print(tag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_label_dict = {}\n",
    "f = open(label_file, \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    if i >= limit_lines:\n",
    "        break\n",
    "    line_array = line.split()\n",
    "#     print(line_array)\n",
    "    picture_name = line_array[1]\n",
    "    # print(picture_name)\n",
    "    classifications = (line_array[12:])[:-1]\n",
    "#     print(classifications)\n",
    "    for i in range(0, len(classifications)): \n",
    "#         classifications[i] = tag_mapping[int(classifications[i])]\n",
    "        classifications[i] = int(classifications[i])\n",
    "    # print(max(classifications))\n",
    "    pic_label_dict[picture_name] = classifications\n",
    "# print(pic_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "#         print(tuple_with_path)\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the labels in the Dataloaders\n",
    "class AvaDataset(Dataset):\n",
    "    def __init__(self, image_path, transform=_transform):\n",
    "        super(AvaDataset, self).__init__()\n",
    "        #TODO\n",
    "    def _find_classes(self, dir):\n",
    "        #TODO\n",
    "    def _get_target(self, file_path):\n",
    "        #TODO\n",
    "    def make_dataset(self, dir, class_to_idx):\n",
    "        #TODO\n",
    "    def get_class_dict(self):\n",
    "        #TODO\n",
    "    def __getitem__(self, index):\n",
    "        #TODO\n",
    "    def __len__(self):\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Define our data transforms to get all our images the same size\n",
    "_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "data = ImageFolderWithPaths(data_dir, transform=_transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data)#, num_workers=4)\n",
    "\n",
    "limit_num_pictures = 1000000\n",
    "\n",
    "valid_size = 0.2 # percentage of data to use for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = data\n",
    "train_data = data\n",
    "num_pictures = len(train_data)\n",
    "\n",
    "# Shuffle pictures and split training set\n",
    "indices = list(range(num_pictures))\n",
    "\n",
    "split = int(np.floor(valid_size * num_pictures))\n",
    "train_idx, test_idx = indices[split:], indices[:split]#rated_indices, bad_indices\n",
    "#print(\"Size of training set: {}, size of test set: {}\".format(len(train_idx), len(test_idx)))\n",
    "\n",
    "# Define samplers that sample elements randomly without replacement\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# Define data loaders, which allow batching and shuffling the data\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "               sampler=train_sampler, batch_size=30)#, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "               sampler=test_sampler, batch_size=30)#, num_workers=4)\n",
    "\n",
    "# check GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# we load the pretrained model, the argument pretrained=True implies to load the ImageNet \n",
    "#     weights for the pre-trained model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.to(device) # loads the model onto the device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False #freeze all convolution weights\n",
    "network = list(vgg16.classifier.children())[:-1] #remove fully connected layer\n",
    "network.extend([nn.Linear(4096, 67)]) #add new layer of 4096->100 (rating scale with 1 decimal - similar to 1 hot encoding)\n",
    "vgg16.classifier = nn.Sequential(*network)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.4, momentum=0.9) # optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.4679,  0.9303,  0.8789,  ...,  0.0569,  0.0912,  0.2453],\n",
      "         [-0.3541,  0.6734,  0.7248,  ...,  0.3994,  0.7933,  0.8276],\n",
      "         [-0.9705,  0.4166,  0.6221,  ...,  0.7419,  0.6049,  1.0673],\n",
      "         ...,\n",
      "         [-1.5357, -1.4843, -1.4843,  ..., -1.3473, -1.7583, -1.5699],\n",
      "         [-1.5870, -1.5699, -1.5185,  ..., -1.6898, -1.6727, -1.5528],\n",
      "         [-1.5528, -1.5357, -1.5014,  ..., -1.7583, -1.5870, -1.5528]],\n",
      "\n",
      "        [[ 0.7479,  1.3782,  1.3957,  ...,  0.4328,  0.4503,  0.5728],\n",
      "         [ 0.0126,  1.2031,  1.2556,  ...,  0.4503,  0.8004,  0.7829],\n",
      "         [-0.5476,  0.9405,  1.2381,  ...,  0.5728,  0.5728,  0.9755],\n",
      "         ...,\n",
      "         [-1.3704, -1.3880, -1.3880,  ..., -1.5105, -1.6506, -1.5280],\n",
      "         [-1.3880, -1.3880, -1.3704,  ..., -1.5980, -1.6331, -1.4580],\n",
      "         [-1.3880, -1.3880, -1.3704,  ..., -1.6681, -1.5630, -1.4405]],\n",
      "\n",
      "        [[ 1.6640,  2.0474,  2.0997,  ...,  1.0539,  0.9668,  1.0714],\n",
      "         [ 1.0365,  1.9428,  2.0300,  ...,  0.6356,  0.8274,  0.9494],\n",
      "         [ 0.6008,  1.8208,  1.9951,  ...,  0.5659,  0.4439,  0.7402],\n",
      "         ...,\n",
      "         [-1.0724, -1.0376, -1.0898,  ..., -1.1944, -1.4384, -1.3339],\n",
      "         [-1.0898, -1.0027, -1.0898,  ..., -1.3861, -1.3687, -1.3339],\n",
      "         [-1.0724, -1.0550, -1.1073,  ..., -1.4559, -1.2816, -1.2816]]]), 1, '/mnt/md0/reynolds/ava-dataset/images/1000.jpg')\n",
      "inputs shape is: torch.Size([30, 3, 224, 224])\n",
      "label shape is: torch.Size([1])\n",
      "label is : tensor([0])\n",
      "output shape is: torch.Size([30, 67]) for image #0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (30) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-460bc260d6bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output shape is: {} for image #{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#         print(output, label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rji3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1836\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (30) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "vgg16.train() # set model to training model\n",
    "num_epochs = 3 \n",
    "training_loss = 0\n",
    "training_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    print(train_loader.dataset[0])\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        #print(i)\n",
    "        for j in range(2): #done so we can utilize both labels\n",
    "            if limit_num_pictures:\n",
    "                if i > limit_num_pictures:\n",
    "                    break\n",
    "            inputs, _, path = data\n",
    "            path = path[0]\n",
    "            path_array = path.split('/')\n",
    "            pic_name = path_array[-1]\n",
    "    #         print(pic_name)\n",
    "    #         print(pic_label_dict[pic_name.split('.')[0]])\n",
    "    #         label = torch.LongTensor(pic_label_dict[pic_name.split('.')[0]])\n",
    "            label = pic_label_dict[pic_name.split('.')[0]][j]\n",
    "#             print(tag_mapping[label])\n",
    "            label = torch.LongTensor([label])\n",
    "            print('inputs shape is: {}'.format(inputs.shape))\n",
    "            print('label shape is: {}'.format(label.shape))\n",
    "            print('label is : {}'.format(label))\n",
    "            optimizer.zero_grad()\n",
    "            output = vgg16(inputs)\n",
    "            print('output shape is: {} for image #{}'.format(output.shape, i))\n",
    "    #         print(output, label)\n",
    "            loss = criterion(output, label)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            num_correct += (preds == pic_label_dict[pic_name.split('.')[0]][j]).sum().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    training_loss = running_loss/len(train_loader.dataset)\n",
    "    training_accuracy = 100 * num_correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg16.state_dict(), 'models/CLASSIFICATION_Feb3_All_AVA_only_training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.load_state_dict(torch.load('models/CLASSIFICATION_Feb3_All_AVA_only_training.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
