{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Images into Subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we shall attempt to classify images into subgroups. This is intended to help rank the images more effectively against other similar images. This would be added to the front of any ranking pipeline. Because of the labels provided we are only able to train with the AVA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SCRIPT IMPORTS\n",
    "'''\n",
    "#standard ML/Image Processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, pandas\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#pytorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# no one likes irrelevant warnings\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# we load the pretrained model, the argument pretrained=True implies to load the ImageNet weights for the pre-trained model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# root directory where the images are stored\n",
    "data_dir = \"/mnt/md0/reynolds/ava-dataset/\"\n",
    "label_file = \"/mnt/md0/reynolds/ava-dataset/AVA_dataset/AVA.txt\"\n",
    "tags_file = \"/mnt/md0/reynolds/ava-dataset/AVA_dataset/tags.txt\"\n",
    "limit_lines = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Abstract', 24: 'Action', 31: 'Advertisement', 66: 'Analog', 19: 'Animals', 20: 'Architecture', 43: 'Astrophotography', 57: 'Birds', 21: 'Black', 51: 'Blur', 64: 'Camera', 16: 'Candid', 50: 'Children', 2: 'Cityscape', 34: 'Digital', 37: 'Diptych', 49: 'DPChallenge', 12: 'Emotive', 4: 'Family', 3: 'Fashion', 63: 'Fish', 38: 'Floral', 40: 'Food', 53: 'High', 45: 'History', 58: 'Horror', 5: 'Humorous', 46: 'Infrared', 65: 'Insects,', 6: 'Interior', 14: 'Landscape', 62: 'Lensbaby', 22: 'Macro', 56: 'Maternity', 44: 'Military', 59: 'Music', 15: 'Nature', 26: 'Nude', 55: 'Overlays', 33: 'Panoramic', 13: 'Performance', 32: 'Persuasive', 52: 'Photo-Impressionism', 25: 'Photojournalism', 60: 'Pinhole/Zone', 30: 'Political', 17: 'Portraiture', 27: 'Rural', 41: 'Science', 35: 'Seascapes', 47: 'Self', 7: 'Sky', 8: 'Snapshot', 9: 'Sports', 18: 'Still', 61: 'Street', 29: 'Studio', 54: 'Texture', 48: 'Textures', 36: 'Traditional', 39: 'Transportation', 23: 'Travel', 10: 'Urban', 11: 'Vintage', 28: 'Water', 42: 'Wedding', 0: 'Miscellaneous'}\n"
     ]
    }
   ],
   "source": [
    "tag_mapping = {}\n",
    "f = open(tags_file, \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    if i >= limit_lines:\n",
    "        break\n",
    "    line_array = line.split()\n",
    "    tag_mapping[int(line_array[0])] = line_array[1]\n",
    "tag_mapping[0] = \"Miscellaneous\"\n",
    "print(tag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_label_dict = {}\n",
    "f = open(label_file, \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    if i >= limit_lines:\n",
    "        break\n",
    "    line_array = line.split()\n",
    "#     print(line_array)\n",
    "    picture_name = line_array[1]\n",
    "    # print(picture_name)\n",
    "    classifications = (line_array[12:])[:-1]\n",
    "#     print(classifications)\n",
    "    for i in range(0, len(classifications)): \n",
    "#         classifications[i] = tag_mapping[int(classifications[i])]\n",
    "        classifications[i] = int(classifications[i])\n",
    "    # print(max(classifications))\n",
    "    pic_label_dict[picture_name] = classifications\n",
    "# print(pic_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "#         print(tuple_with_path)\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data = ImageFolderWithPaths(data_dir, transform=_transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(data)#, num_workers=4)\n",
    "\n",
    "limit_num_pictures = 1000000\n",
    "\n",
    "# Define our data transforms to get all our images the same size\n",
    "_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "valid_size = 0.2 # percentage of data to use for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = data\n",
    "train_data = data\n",
    "num_pictures = len(train_data)\n",
    "\n",
    "# Shuffle pictures and split training set\n",
    "indices = list(range(num_pictures))\n",
    "\n",
    "split = int(np.floor(valid_size * num_pictures))\n",
    "train_idx, test_idx = indices[split:], indices[:split]#rated_indices, bad_indices\n",
    "#print(\"Size of training set: {}, size of test set: {}\".format(len(train_idx), len(test_idx)))\n",
    "\n",
    "# Define samplers that sample elements randomly without replacement\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# Define data loaders, which allow batching and shuffling the data\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "               sampler=train_sampler, batch_size=1)#, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "               sampler=test_sampler, batch_size=1)#, num_workers=4)\n",
    "\n",
    "# check GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# we load the pretrained model, the argument pretrained=True implies to load the ImageNet \n",
    "#     weights for the pre-trained model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.to(device) # loads the model onto the device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False #freeze all convolution weights\n",
    "network = list(vgg16.classifier.children())[:-1] #remove fully connected layer\n",
    "network.extend([nn.Linear(4096, 67)]) #add new layer of 4096->100 (rating scale with 1 decimal - similar to 1 hot encoding)\n",
    "vgg16.classifier = nn.Sequential(*network)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.4, momentum=0.9) # optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.train() # set model to training model\n",
    "num_epochs = 3 \n",
    "training_loss = 0\n",
    "training_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        #print(i)\n",
    "        for j in range(2): #done so we can utilize both labels\n",
    "            if limit_num_pictures:\n",
    "                if i > limit_num_pictures:\n",
    "                    break\n",
    "            inputs, _, path = data\n",
    "            path = path[0]\n",
    "            path_array = path.split('/')\n",
    "            pic_name = path_array[-1]\n",
    "    #         print(pic_name)\n",
    "    #         print(pic_label_dict[pic_name.split('.')[0]])\n",
    "    #         label = torch.LongTensor(pic_label_dict[pic_name.split('.')[0]])\n",
    "            label = pic_label_dict[pic_name.split('.')[0]][j]\n",
    "#             print(tag_mapping[label])\n",
    "            label = torch.LongTensor([label])\n",
    "#             print('inputs shape is: {}'.format(inputs.shape))\n",
    "#             print('label shape is: {}'.format(label.shape))\n",
    "#             print('label is : {}'.format(label))\n",
    "            optimizer.zero_grad()\n",
    "            output = vgg16(inputs)\n",
    "    #         print('output shape is: {}'.format(output.shape))\n",
    "    #         print(output, label)\n",
    "            loss = criterion(output, label)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            num_correct += (preds == pic_label_dict[pic_name.split('.')[0]][j]).sum().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    training_loss = running_loss/len(train_loader.dataset)\n",
    "    training_accuracy = 100 * num_correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg16.state_dict(), 'models/CLASSIFICATION_Feb3_All_AVA_only_training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.load_state_dict(torch.load('models/CLASSIFICATION_Feb3_All_AVA_only_training.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
